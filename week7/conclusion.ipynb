{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af2bf58-be64-4a6c-8bee-591356bc7afb",
   "metadata": {},
   "source": [
    "# Autocompletion\n",
    "\n",
    "## N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e405bbc-1331-4896-8660-c556e0ddc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "nltk.data.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b76609-58bc-416a-8ba4-933767399451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"en_US.twitter.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "data[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5ae76-5862-4c5c-88d7-0d5e49be4149",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c5b94c-78eb-4dac-8dc4-e4d10ca4656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    return (sentence.strip() for sentence in data.split(os.linesep) if len(sentence.strip()) > 0)\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    return (nltk.word_tokenize(sentence.lower()) for sentence in sentences)\n",
    "\n",
    "\n",
    "def tokenize_data(data):\n",
    "    return list(tokenize_sentences(split_data(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79448265-cc3d-4902-a447-f7b817b18fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['how',\n",
       "  'are',\n",
       "  'you',\n",
       "  '?',\n",
       "  'btw',\n",
       "  'thanks',\n",
       "  'for',\n",
       "  'the',\n",
       "  'rt',\n",
       "  '.',\n",
       "  'you',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'be',\n",
       "  'in',\n",
       "  'dc',\n",
       "  'anytime',\n",
       "  'soon',\n",
       "  '?',\n",
       "  'love',\n",
       "  'to',\n",
       "  'see',\n",
       "  'you',\n",
       "  '.',\n",
       "  'been',\n",
       "  'way',\n",
       "  ',',\n",
       "  'way',\n",
       "  'too',\n",
       "  'long',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'you',\n",
       "  'meet',\n",
       "  'someone',\n",
       "  'special',\n",
       "  '...',\n",
       "  'you',\n",
       "  \"'ll\",\n",
       "  'know',\n",
       "  '.',\n",
       "  'your',\n",
       "  'heart',\n",
       "  'will',\n",
       "  'beat',\n",
       "  'more',\n",
       "  'rapidly',\n",
       "  'and',\n",
       "  'you',\n",
       "  \"'ll\",\n",
       "  'smile',\n",
       "  'for',\n",
       "  'no',\n",
       "  'reason',\n",
       "  '.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = tokenize_data(data)\n",
    "tokenized_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ae221-70ff-4638-8532-ee1646cdfa6f",
   "metadata": {},
   "source": [
    "### Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fd95d41-2fdd-46eb-91e2-db9d40e6a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size=38368\n",
      "test set size=9593\n",
      "total=47961\n"
     ]
    }
   ],
   "source": [
    "train_set_share = 0.8\n",
    "train_set_size = int(len(tokenized_data) * 0.8)\n",
    "\n",
    "train_set = tokenized_data[:train_set_size]\n",
    "test_set = tokenized_data[train_set_size:]\n",
    "\n",
    "print(\n",
    "    os.linesep.join(\n",
    "        [f\"train set size={len(train_set)}\", f\"test set size={len(test_set)}\", f\"total={len(tokenized_data)}\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3a05a-ed12-450d-9f3a-f369583aedc3",
   "metadata": {},
   "source": [
    "### Using Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a1a5469-cd20-410c-88af-cc3990dc56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_vocabularies(tokenized_sentences, threshold=2):\n",
    "    word_map = defaultdict(lambda: 0)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            word_map[word] += 1\n",
    "\n",
    "    return {word: count for word, count in word_map.items() if count > threshold}\n",
    "\n",
    "\n",
    "def replace_rare_words(tokenized_sentences, vocabularies, unknown_word_token=\"<UNK>\"):\n",
    "    replaced_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence_copy = []\n",
    "        for word in sentence:\n",
    "            sentence_copy.append(word if word in vocabularies else unknown_word_token)\n",
    "        replaced_sentences.append(sentence_copy)\n",
    "\n",
    "    return replaced_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27548d8-ca71-4363-b720-086f389df3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularies = get_vocabularies(train_set)\n",
    "train_set_2 = replace_rare_words(train_set, vocabularies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f10bf588-a2ac-4573-88f9-9132a5cc29c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'always',\n",
       "  'wonder',\n",
       "  'how',\n",
       "  'the',\n",
       "  'guys',\n",
       "  'on',\n",
       "  'the',\n",
       "  'auctions',\n",
       "  'shows',\n",
       "  'learned',\n",
       "  'to',\n",
       "  'talk',\n",
       "  'so',\n",
       "  'fast',\n",
       "  '!',\n",
       "  '?',\n",
       "  'all',\n",
       "  'i',\n",
       "  'hear',\n",
       "  'is',\n",
       "  '<UNK>',\n",
       "  '.'],\n",
       " ['<UNK>', 'what', 'a', 'catch']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_2[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b4c307-4976-4d18-8ef2-46e69cb5bcec",
   "metadata": {},
   "source": [
    "### Counting N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08d8f97b-1402-442e-9afb-f07599f11cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(tokenized_sentences, n, starting_token=\"<S>\", ending_token=\"<E>\"):\n",
    "    n_gram_counts = defaultdict(lambda: 0)\n",
    "    for sentence in tokenized_sentences:\n",
    "        sentence = tuple([starting_token] * n + sentence + [ending_token])\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            n_gram = sentence[i : i + n]\n",
    "            n_gram_counts[n_gram] += 1\n",
    "    return n_gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb75241-003e-4c68-b876-6305a792a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = count_n_grams(train_set_2, 2)\n",
    "trigram_counts = count_n_grams(train_set_2, 3)\n",
    "trigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900279d-2663-48c2-9b04-233e21652058",
   "metadata": {},
   "source": [
    "### Estimate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f130549e-f3d1-4d8e-8960-c9d3f1f23346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus_1_gram_counts, vocabularies_size, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "\n",
    "    n_plus_1_gram = (*previous_n_gram, word)\n",
    "    n_plus_1_gram_count = n_plus_1_gram_counts.get(n_plus_1_gram, 0)\n",
    "\n",
    "    return (n_plus_1_gram_count + k) / (previous_n_gram_count + k * vocabularies_size)\n",
    "\n",
    "\n",
    "def estimate_probabilities(\n",
    "    previous_n_gram,\n",
    "    n_gram_counts,\n",
    "    n_plus_1_gram_counts,\n",
    "    vocabularies,\n",
    "    ending_token=\"<E>\",\n",
    "    unknown_word_token=\"<UNK>\",\n",
    "    k=1.0,\n",
    "):\n",
    "    words = list(vocabularies.keys()) + [ending_token, unknown_word_token]\n",
    "    return {\n",
    "        word: estimate_probability(word, previous_n_gram, n_gram_counts, n_plus_1_gram_counts, len(vocabularies), k)\n",
    "        for word in words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3e658c9-d604-48e7-840f-26ab36e5a1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are (?)\n",
      "most likely: how are you\n"
     ]
    }
   ],
   "source": [
    "probs = estimate_probabilities((\"how\", \"are\"), bigram_counts, trigram_counts, vocabularies)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "most_likely = Counter(probs).most_common(1)[0][0]\n",
    "\n",
    "print(\"how are (?)\")\n",
    "print(f\"most likely: how are {most_likely}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
